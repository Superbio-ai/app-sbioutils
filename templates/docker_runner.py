import logging
import subprocess
import traceback
import time
import sys
import json
from os.path import exists
from sbioapputils.docker_runner.workflow_utils import parse_workflow, remove_empty_keys, get_workflow_loc
from sbioapputils.app_runner.dev_utils import payload_from_yaml
from sbioapputils.app_runner.app_runner_utils import AppRunnerUtils


def _get_yaml(job_id: str, request: dict):
    workflow_name, workflow_dest = get_workflow_loc(job_id, request)
    source_file_path = f'apps/{job_id}/resources/{workflow_name}'
    AppRunnerUtils.download_file(source_file_path, workflow_dest)
    ################ n.b. might need to clear dest every so often as could build up with files/folders. Should be a better way to do this ###############


def _process_stage(stage_name, command):
    logging.info(f'Stage {stage_name} starting')
    start_time = time.time()
    
    process = subprocess.Popen(command, stdout=subprocess.PIPE)
    while True:
        line = process.stdout.readline()
        if not line:
            break
        logging.info(line.rstrip())

    if process.returncode is not None:
        logging.info(f"Error occurred in subprocess {stage_name}")
        logging.info(process.returncode)
        raise Exception(f"Error occurred in subprocess {stage_name}, with code {process.returncode}")

    end_time = time.time()
    logging.info(f'Stage {stage_name} completed in {end_time - start_time} seconds')


def _upload_results(job_id: str, request: dict):
    _, workflow_loc = get_workflow_loc(job_id, request)
    
    #reads payload json if generated by code, otherwise gets from yaml
    if (exists('/resources/results_for_payload.json')) and (exists('/resources/results_for_upload.json')):
        logging.info("Generating payload from custom json")
        with open('/resources/results_for_payload.json', 'r') as f:
            results_for_payload = json.load(f)
        with open('/resources/results_for_upload.json', 'r') as f:
            results_for_upload = json.load(f)
    else:
        logging.info("Generating payload from yaml file")
        results_for_payload, results_for_upload = payload_from_yaml(workflow_loc)
    results_for_payload = remove_empty_keys(results_for_payload)
    
    #upload results
    logging.info('Payload:')
    logging.info(results_for_payload)

    AppRunnerUtils.upload_results(job_id, results_for_payload)
    logging.info('Additional artifacts:')
    logging.info(results_for_upload)
    for element in results_for_upload:
        AppRunnerUtils.upload_file(job_id, element)
    AppRunnerUtils.set_job_completed(job_id, results_for_payload)


def main():
    job_log_file = 'job.log'
    AppRunnerUtils.set_logging(job_log_file)
    job_id = sys.argv[1]
    try:
        request = AppRunnerUtils.get_job_config(job_id)
        _get_yaml(job_id, request)
        stages, request_errors = parse_workflow(job_id, request)
        if request_errors:
            raise Exception(f"Invalid json request:\n {request_errors}")
        else:
            logging.info('Workflow parsed')
            logging.info(f'Job config: {request}')
        
        AppRunnerUtils.set_job_running(job_id)
        logging.info(f'Job {job_id} is running')
        
        for stage, command in stages.items():
            _process_stage(stage, command)
        _upload_results(job_id, request)
        
    except Exception as e:
        err = str(e)
        AppRunnerUtils.set_job_failed(job_id, err)
        logging.error(traceback.format_exc())
        
    finally:
        # upload log files to S3
        AppRunnerUtils.upload_file(job_id, job_log_file)


if __name__ == '__main__':
    main()
